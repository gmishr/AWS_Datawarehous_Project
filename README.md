# Sparkify Database
## Introduction:
A startup called Sparkify wants to analyze the data they've been collecting on songs and user activity on their new music streaming app. So in this Sparkify database we are loading two source files song data and log data placed in S3 bucket into one fact table and four dimension tables in redshift using ETL written in python. So that analytics team can easily access normalized data to get insights from that data which will help business to take decisions.

## Source Dataset:
### Song Dataset: 
Each file is in JSON format and contains metadata about a song and the artist of that song.The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset.
File Examples: 
song_data/A/B/C/TRABCEI128F424C983.json
song_data/A/A/B/TRAABJL12903CDCF1A.json

### Log Dataset: 
Dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations. This file partitioned by year and month.
File Examples:
log_data/2018/11/2018-11-12-events.json
log_data/2018/11/2018-11-13-events.json

## Database Schema:
### Stagging Table:
Name : stg_songs
In this table we are loading data from song files stored in s3 bucket and it contains below columns.
num_songs,artist_id, artist_latitude,artist_longitude,artist_location,artist_name,song_id,title,duration,year

Name : stg_events

In this table we are loading data from event files stored in s3 bucket and it contains below columns.
artist,auth ,first_name,gender ,item_in_session,last_name ,length ,level,location,method,page,registration,session_id,song,status,ts,user_agent,user_id 

### Fact Table:
Name : songplays 
In this table we loading log data associated with song plays and it contains below columns.
songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent

### Dimension Tables:
Name : users 
In this table we are loading data related to user which is getting loaded from stg_events table and it contains below columns.
user_id, first_name, last_name, gender, level

Name : songs
In this table we are loading data related to song which is getting loaded from stg_songs table and it contains below columns.
song_id, title, artist_id, year, duration

Name : artists
In this table we are loading data related to artist which is getting loaded from stg_songs table and it contains below columns.
artist_id, name, location, latitude, longitude

Name : time 
In this table we are loading data related to timestamp and all columns in this table are extracted from one column timestamp of stg_events tsble and it contains below columns.
start_time, hour, day, week, month, year, weekday

## Project Code Files:
### sql_queries.py:
This file contains all SQL queries which are required to create tables, drop tables, insert data into tables, select data from tables which we are using to load data in all tables from source files of S3 to  stagging redshift table and from stagging to  final redshift tables.

### create_tables.py:
This file contains python script which is used to create database and tables by importing queries written in  sql_queries.py script.

### etl.py:
This script code is written to reads and processes files from song_data and log_data and loads them into stagging and from stagging to fact and dimension tables with help of insert and select quries written in sql_queries.py.

## Commands:
### Command to install required packages:
#### to install psycopg2 package:
pip install psycopg2

### Command to create Database and Tables:
python create_tables.py

### Command to load data into tables from source dataset:
python etl.py